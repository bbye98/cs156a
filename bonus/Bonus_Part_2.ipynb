{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import (Activation, BatchNormalization, Conv2D, \n",
        "                          Conv2DTranspose, Dense, Dropout, Flatten, Input, \n",
        "                          LeakyReLU, Reshape, UpSampling2D)\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm \n",
        "\n",
        "matplotlib.use('Agg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhSCHnvhJImu"
      },
      "source": [
        "## Deep Convolutional GAN\n",
        "\n",
        "Author: George Stathopoulos\n",
        "\n",
        "Last modified: April 4, 2022\n",
        "\n",
        "Description: A script to train a GAN on MNIST. Generates image samples from the generator and saves to a local directory, does not save the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oco6V6U3C-jl"
      },
      "outputs": [],
      "source": [
        "# Some constants\n",
        "MNIST_SIZE = 28\n",
        "LATENT_DIM = 100\n",
        "\n",
        "## Return MNIST dataset, shaped appropriately depending on whether we are\n",
        "## want to train a dense or convolutional neural net\n",
        "def get_data():\n",
        "    # Import the MNIST dataset using Keras\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Normalize input images to have all values in [0, 1]\n",
        "    # Reshape image dataset to have shape (N, H, W, C) instead of (N, H, W)\n",
        "    X_train = X_train.reshape((*(X_train.shape), 1)) / 255\n",
        "    X_test = X_test.reshape((*(X_test.shape), 1)) / 255\n",
        "\n",
        "    # Convert labels to one-hot vectors (probability distributions w/\n",
        "    # probability 1 assigned to the correct label)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "def make_generator(num_filters=64, num_hidden_conv_layers=2, init_dim=7):\n",
        "    gen = Sequential()\n",
        "    # Model input is a feature vector of size 100\n",
        "    gen.add(Dense(init_dim**2 * num_filters, input_dim=LATENT_DIM))\n",
        "    gen.add(Activation('relu'))\n",
        "    gen.add(Reshape((init_dim, init_dim, num_filters)))\n",
        "\n",
        "    for _ in range(num_hidden_conv_layers):\n",
        "        # Input: d x d x k\n",
        "        # Output 2d x 2d x k/2\n",
        "        if (init_dim < MNIST_SIZE):\n",
        "            gen.add(UpSampling2D())\n",
        "            init_dim *= 2\n",
        "        num_filters //= 2\n",
        "        gen.add(Conv2DTranspose(num_filters, 5, padding='same'))\n",
        "        gen.add(BatchNormalization(momentum=0.4))\n",
        "        gen.add(Activation('relu'))\n",
        "\n",
        "    gen.add(Conv2DTranspose(1, 5, padding='same'))\n",
        "    gen.add(Activation('sigmoid'))\n",
        "    # Output should be 28 x 28 x 1\n",
        "    # gen.summary()\n",
        "    return gen\n",
        "\n",
        "def make_discriminator(num_filters=32, num_hidden_layers=3, dropout=0.3):\n",
        "    d = Sequential()\n",
        "\n",
        "    d.add(Conv2D(num_filters*1, 5, strides=2,\n",
        "                 input_shape=(MNIST_SIZE, MNIST_SIZE, 1), padding='same'))\n",
        "    d.add(LeakyReLU()) # leakyrelu so generator has derivative\n",
        "    d.add(Dropout(dropout))\n",
        "\n",
        "    for i in range(1, num_hidden_layers):\n",
        "        # Powers of 2 are generally better suited for GPU\n",
        "        d.add(Conv2D(num_filters * 2**i, 5, strides=2, padding='same'))\n",
        "        d.add(LeakyReLU())\n",
        "        d.add(Dropout(dropout))\n",
        "\n",
        "    # NOTE: Difference between this and build_conv_net\n",
        "    #       is that there is only a SINGLE output class,\n",
        "    #       which corresponds to FAKE/REAL.\n",
        "    d.add(Flatten())\n",
        "    d.add(Dense(1))\n",
        "    d.add(Activation('sigmoid'))\n",
        "    d.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return d\n",
        "\n",
        "def make_adversial_network(generator, discriminator):\n",
        "    # This will only be used for training the generator.\n",
        "    # Note, the weights in the discriminator and generator are shared.\n",
        "    discriminator.trainable = False\n",
        "    gan = Sequential([generator, discriminator])\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan #, generator, discriminator\n",
        "\n",
        "def generate_latent_noise(n):\n",
        "    return np.random.uniform(-1, 1, size=(n, LATENT_DIM))\n",
        "\n",
        "def visualize_generator(epoch, generator,\n",
        "                        num_samples=100, dim=(10,10),\n",
        "                        figsize=(10,10), path=''):\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        img = generator.predict(generate_latent_noise(1))[0, :, :, 0]\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'generator_samples/gan_epoch_{epoch}.png')\n",
        "    plt.close()\n",
        "\n",
        "def train(epochs=1, batch_size=128, path=''):\n",
        "    # Import the MNIST dataset using Keras, will only\n",
        "    # use the 60,000 training examples.\n",
        "    (X_train, _), _ = get_data()\n",
        "\n",
        "    # Creating GAN\n",
        "    generator = make_generator()\n",
        "    discriminator = make_discriminator()\n",
        "    adversial_net = make_adversial_network(generator, discriminator)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    visualize_generator(0, generator, path=path)\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        print(f'Epoch {epoch + 1}')\n",
        "\n",
        "        discr_loss = 0\n",
        "        gen_loss = 0\n",
        "        for _ in tqdm(range(batch_size)):\n",
        "            noise = generate_latent_noise(batch_size)\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            real_images = X_train[np.random.choice(X_train.shape[0], batch_size,\n",
        "                                                   replace=False)]\n",
        "\n",
        "            discrimination_data = np.concatenate([real_images, generated_images])\n",
        "\n",
        "            # Labels for generated and real data, uses soft label trick\n",
        "            discrimination_labels = 0.1 * np.ones(2 * batch_size)\n",
        "            discrimination_labels[:batch_size] = 0.9\n",
        "\n",
        "            # To train, we alternate between training just the discriminator\n",
        "            # and just the generator.\n",
        "            discriminator.trainable = True\n",
        "            discr_loss += discriminator.train_on_batch(discrimination_data,\n",
        "                                                       discrimination_labels)\n",
        "\n",
        "            # Trick to 'freeze' discriminator weights in adversial_net. Only\n",
        "            # the generator weights will be changed, which are shared with\n",
        "            # the generator.\n",
        "            discriminator.trainable = False\n",
        "            # N.B, changing the labels because now we want to 'fool' the\n",
        "            # discriminator.\n",
        "            gen_loss += adversial_net.train_on_batch(noise, np.ones(batch_size))\n",
        "\n",
        "        print(f'Discriminator Loss: {discr_loss / batch_size}')\n",
        "        print(f'Generator Loss: {gen_loss / batch_size}')\n",
        "        print(time.time() - start)\n",
        "        visualize_generator(epoch + 1, generator, path=path)\n",
        "\n",
        "        print((time.time() - start) / 3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwaPFcjpDZfr"
      },
      "outputs": [],
      "source": [
        "os.makedirs(os.path.join(os.getcwd(), 'generator_samples'), exist_ok=True)\n",
        "train(epochs=150)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bonus_Part_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
