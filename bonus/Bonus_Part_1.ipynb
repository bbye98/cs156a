{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import tensorrt\n",
        "import keras.backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Activation, Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XbMbt_JVXZ1"
      },
      "source": [
        "## Visualize\n",
        "Author: Aadyot Bhatnagar\n",
        "\n",
        "Last modified: March 31, 2022\n",
        "\n",
        "Description: A script to visualize some examples from the MNIST dataset of handwritten digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZjtJai2PRjn"
      },
      "outputs": [],
      "source": [
        "train, val = mnist.load_data()\n",
        "nrow, ncol = 3, 5\n",
        "\n",
        "for data, label, kind in [(*train, 'Training'), (*val, 'Validation')]:\n",
        "    for i in range(nrow):\n",
        "        for j in range(ncol):\n",
        "            idx = i * ncol + j\n",
        "            plt.subplot(nrow, ncol, idx + 1)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(data[idx], cmap='gray')\n",
        "            plt.title(f'Label: {label[idx]}')\n",
        "\n",
        "    plt.suptitle(f'Example {kind} Images')\n",
        "    plt.savefig(f'{kind.lower()}_example.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPuvYfuXDuEz"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqFSy4ALDtfW"
      },
      "outputs": [],
      "source": [
        "FILE_PREFIX = 'dense_arch1'   # The model is saved in a file with the provided prefix under the folder icon on the left\n",
        "DENSE_NET = True              # True trains a dense NN, False trains a convolutional NN\n",
        "REGULARIZATION = 0.00         # l2 regularization strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wzgm-qBaLtJ"
      },
      "source": [
        "## Train\n",
        "Author: Aadyot Bhatnagar\n",
        "\n",
        "Last modified: April 19, 2022\n",
        "\n",
        "Description: A script to train and save a neural net to recognize the MNIST dataset of handwritten digits. Supports both a standard dense network and a convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JARYJkVVeGjI"
      },
      "outputs": [],
      "source": [
        "## Return MNIST dataset, shaped appropriately depending on whether we are\n",
        "## want to train a dense or convolutional neural net\n",
        "def get_data(is_net_conv):\n",
        "    # Import the MNIST dataset using Keras\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Normalize input images to have all values in [0, 1]\n",
        "    # Reshape image dataset to have shape (N, H, W, C) instead of (N, H, W)\n",
        "    if is_net_conv:\n",
        "        X_train = X_train.reshape((*(X_train.shape), 1)) / 255\n",
        "        X_test = X_test.reshape((*(X_test.shape), 1)) / 255\n",
        "\n",
        "    # Normalize input images to have all values in [0, 1]\n",
        "    # Flatten image dataset to have shape (N, H * W) instead of (N, H, W)\n",
        "    else:\n",
        "        X_train = X_train.reshape((X_train.shape[0], -1)) / 255\n",
        "        X_test = X_test.reshape((X_test.shape[0], -1)) / 255\n",
        "\n",
        "    # Convert labels to one-hot vectors (probability distributions w/\n",
        "    # probability 1 assigned to the correct label)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "\n",
        "## Construct a dense neural net and return it\n",
        "def build_dense_net(reg_param, *, ns=[200, 100], activation='relu', regularizer=l2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(ns[0], input_shape=(784,), kernel_regularizer=regularizer(reg_param)))\n",
        "    model.add(Activation(activation))\n",
        "    for n in ns[1:]:\n",
        "        model.add(Dense(n, kernel_regularizer=regularizer(reg_param)))\n",
        "        model.add(Activation(activation))\n",
        "    model.add(Dense(10, kernel_regularizer=regularizer(reg_param)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "## Construct a convolutional neural net and return it\n",
        "def build_conv_net(reg_param):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(20, (5, 5), padding='same',\n",
        "                     input_shape=(28, 28, 1),\n",
        "                     kernel_regularizer=l2(reg_param)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(10, (3, 3), padding='same', \n",
        "                     kernel_regularizer=l2(reg_param)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Need to flatten tensor output from conv layer to vector for dense layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, kernel_regularizer=l2(reg_param)))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def main(\n",
        "        model_name, dense_net=True, regularization=0.00, *, \n",
        "        ns=[200, 100], activation=\"relu\", regularizer=l2, \n",
        "        path=\"\", save=True):\n",
        "    \"\"\"\n",
        "    Trains a NN to recognize handwritten digits.\n",
        "\n",
        "    file_prefix - prefix for file to save trained model to (e.g. dense_arch1, conv_regularize05, etc.)\n",
        "    dense_net - whether to train a dense NN or a convolutional NN (defaults to dense NN)\n",
        "    regularization - strength of l2 regularization to use (defaults to no regularization)\n",
        "    \"\"\"\n",
        "\n",
        "    file_prefix = model_name\n",
        "    if path:\n",
        "        # file_prefix = f\"{model_name}_{n1=}_{n2=}\"\n",
        "        file_prefix = f\"{model_name}_ns={','.join(map(str, ns))}\"\n",
        "        if regularization:\n",
        "            file_prefix += f\"_L={regularization:.0e}\"\n",
        "        if activation != \"relu\":\n",
        "            file_prefix += f\"_{activation}\"\n",
        "        if regularizer != l2:\n",
        "            file_prefix += f\"_{regularizer.__name__}\"\n",
        "\n",
        "    # Importing the MNIST dataset using Keras\n",
        "    (X_train, y_train), (X_test, y_test) = get_data(not dense_net)\n",
        "    model = build_dense_net(regularization, ns=ns, activation=activation, \n",
        "                            regularizer=regularizer) if dense_net \\\n",
        "            else build_conv_net(regularization)\n",
        "\n",
        "    # Print a summary of the layers and weights in the model\n",
        "    model.summary()\n",
        "\n",
        "    # Have our model minimize the categorical cross entropy loss with the adam\n",
        "    # optimizer (fancier stochastic gradient descent that converges faster)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    history = model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1,\n",
        "                        validation_data=(X_test, y_test))\n",
        "\n",
        "    if save:\n",
        "        cwd = os.getcwd()\n",
        "\n",
        "        # Create img directory to save images if needed\n",
        "        os.makedirs(os.path.join(cwd, path, 'img'), exist_ok=True)\n",
        "        plot_fname = os.path.join(cwd, path, 'img', f'{file_prefix}_learn.png')\n",
        "\n",
        "        # Create model directory to save models if needed\n",
        "        os.makedirs(os.path.join(cwd, path, 'model'), exist_ok=True)\n",
        "        model_weights_fname = os.path.join(cwd, path, 'model', f'{file_prefix}.h5')\n",
        "        model_json_fname = os.path.join(cwd, path, 'model', f'{file_prefix}.json')\n",
        "        \n",
        "        # Save model weights and json spec describing the model's architecture\n",
        "        model.save(model_weights_fname)\n",
        "        with open(model_json_fname, 'w') as f:\n",
        "            f.write(json.dumps(json.loads(model.to_json()), indent=4))\n",
        "\n",
        "    # Plot accuracy learning curve\n",
        "    _, axs = plt.subplots(2, 1, sharex=True)\n",
        "    axs[0].plot(history.history['accuracy'])\n",
        "    axs[0].plot(history.history['val_accuracy'])\n",
        "    axs[0].set_title(f'{model_name} accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "    # Plot loss learning curve\n",
        "    axs[1].plot(history.history['loss'])\n",
        "    axs[1].plot(history.history['val_loss'])\n",
        "    axs[1].set_title(f'{model_name} loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "    if path:\n",
        "        legend = \"\"\n",
        "        if len(ns) == 2 and (ns[0] != 200 or ns[1] != 100):\n",
        "            legend += f\"d_1={ns[0]},\\,d_2={ns[1]}\"\n",
        "        if regularization:\n",
        "            if len(legend):\n",
        "                legend += \",\\,\"\n",
        "            legend += f\"\\lambda=10^{{{np.log10(regularization):.0f}}}\"\n",
        "        print(legend)\n",
        "        axs[0].set_ylim((0.8, 1))\n",
        "        axs[1].set_ylim((0, 2 / 3))\n",
        "        axs[0].text(-2, 1.03, f\"${legend}$\" if legend else \"\")\n",
        "    plt.tight_layout()\n",
        "    if save:\n",
        "        plt.savefig(plot_fname)\n",
        "    plt.show()\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main(FILE_PREFIX, DENSE_NET, REGULARIZATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 Classifying digits with NNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mpl.rcParams.update(\n",
        "    {\n",
        "        \"axes.labelsize\": 14,\n",
        "        \"figure.autolayout\": True,\n",
        "        \"figure.figsize\": (4.875, 3.65625),\n",
        "        \"font.size\": 12,\n",
        "        \"legend.columnspacing\": 1,\n",
        "        \"legend.edgecolor\": \"1\",\n",
        "        \"legend.framealpha\": 0,\n",
        "        \"legend.fontsize\": 12,\n",
        "        \"legend.handlelength\": 1.25,\n",
        "        \"legend.labelspacing\": 0.25,\n",
        "        \"xtick.labelsize\": 12,\n",
        "        \"ytick.labelsize\": 12,\n",
        "        \"text.usetex\": True\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Number of Parameters\n",
        "\n",
        "Author: Benjamin Ye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'dense_arch1'\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if os.path.isfile(os.path.join(cwd, 'exercise/1.3', f'{model_name}_stats.pkl')):\n",
        "    df = pd.read_pickle(os.path.join(cwd, 'exercise/1.3', f'{model_name}_stats.pkl'))\n",
        "else:\n",
        "    cols = {'n1': int, 'n2': int, 'n_params': int, 'train_accuracy': float,\n",
        "            'train_loss': float, 'validation_accuracy': float, \n",
        "            'validation_loss': float}\n",
        "    df = pd.DataFrame({c: pd.Series(dtype=t) for c, t in cols.items()})\n",
        "\n",
        "    for n1 in (25, 50, 100, 200, 400):\n",
        "        for n2 in (25, 50, 100, 200):\n",
        "            model, history = main(model_name, ns=[n1, n2], path='exercise/1.3')\n",
        "            df.loc[len(df)] = [\n",
        "                n1, n2, \n",
        "                sum(K.count_params(p) for p in model.trainable_weights)\n",
        "                + sum(K.count_params(p) for p in model.non_trainable_weights),\n",
        "                history.history['accuracy'][-1], \n",
        "                history.history['loss'][-1],\n",
        "                history.history['val_accuracy'][-1], \n",
        "                history.history['val_loss'][-1]\n",
        "            ]\n",
        "\n",
        "    model, history = main(model_name, ns=[47, 25], path='exercise/1.3')\n",
        "    df.loc[len(df)] = [\n",
        "        n1, n2, \n",
        "        sum(K.count_params(p) for p in model.trainable_weights)\n",
        "        + sum(K.count_params(p) for p in model.non_trainable_weights),\n",
        "        history.history['accuracy'][-1], \n",
        "        history.history['loss'][-1],\n",
        "        history.history['val_accuracy'][-1], \n",
        "        history.history['val_loss'][-1]\n",
        "    ]\n",
        "    df.to_pickle(os.path.join(cwd, 'exercise/1.3', f'{model_name}_stats.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'dense_arch1'\n",
        "cwd = os.getcwd()\n",
        "\n",
        "if os.path.isfile(os.path.join(cwd, 'exercise/1.4', f'{model_name}_reg_stats.pkl')):\n",
        "    df_reg = pd.read_pickle(os.path.join(cwd, 'exercise/1.4', f'{model_name}_reg_stats.pkl'))\n",
        "else:\n",
        "    cols = {'lambda': int, 'n_params': int, 'train_accuracy': float,\n",
        "            'train_loss': float, 'validation_accuracy': float, \n",
        "            'validation_loss': float}\n",
        "    df_reg = pd.DataFrame({c: pd.Series(dtype=t) for c, t in cols.items()})\n",
        "\n",
        "    for L in (10.0 ** np.arange(-10, 2, dtype=int)):\n",
        "        model, history = main(model_name, regularization=L, path='exercise/1.4')\n",
        "        df_reg.loc[len(df_reg)] = [\n",
        "            L,\n",
        "            sum(K.count_params(p) for p in model.trainable_weights)\n",
        "            + sum(K.count_params(p) for p in model.non_trainable_weights),\n",
        "            history.history['accuracy'][-1], \n",
        "            history.history['loss'][-1],\n",
        "            history.history['val_accuracy'][-1], \n",
        "            history.history['val_loss'][-1]\n",
        "        ]\n",
        "    df_reg.to_pickle(os.path.join(cwd, 'exercise/1.4', f'{model_name}_reg_stats.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for L in (10.0 ** np.arange(-10, -7, dtype=int)):\n",
        "    main(model_name, regularization=L, ns=[47, 25], path='exercise/1.3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, history = main(model_name, activation=\"tanh\", path='exercise/1.5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6 Different Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'dense_arch1'\n",
        "for ns in ([100, 100], [150, 50], [175, 25], [199, 0]):\n",
        "    main(model_name, ns=ns, path='exercise/1.6')\n",
        "    main(model_name, ns=ns, regularization=1e-8, regularizer=l1, path='exercise/1.6')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7 Convolutional Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main(model_name, False, path='exercise/1.7')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "Author: Aadyot Bhatnagar\n",
        "\n",
        "Last modified: March 31, 2022\n",
        "\n",
        "Description: A script to load and evaluate a saved Keras model's performance on the MNIST dataset of handwritten images. Prints out training and validation loss and accuracy, and also visualizes validation images the model got wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Get data in a format compatible with the neural net we want to evaluate\n",
        "def get_data(model):\n",
        "    # Import the MNIST dataset using Keras\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Determine input shape that the model given should take\n",
        "    input_shape = model.get_layer(index=0).input_shape\n",
        "\n",
        "    # Normalize data to be in [0, 1] and reshape appropriately\n",
        "    X_train = X_train.reshape(-1, *input_shape[1:]) / 255\n",
        "    X_test = X_test.reshape(-1, *input_shape[1:]) / 255\n",
        "\n",
        "    # Convert labels to one-hot vectors (probability distributions w/\n",
        "    # probability 1 assigned to the correct label)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "\n",
        "def main(file_prefix):\n",
        "    model_name = file_prefix\n",
        "\n",
        "    # Remove src from cwd if necessary\n",
        "    cwd = os.getcwd()\n",
        "    if os.path.basename(cwd) == 'src': \n",
        "        cwd = os.path.dirname(cwd)\n",
        "\n",
        "    # Create img directory to save images if needed\n",
        "    os.makedirs(os.path.join(cwd, 'img'), exist_ok=True)\n",
        "\n",
        "    # Create model directory to save models if needed\n",
        "    os.makedirs(os.path.join(cwd, 'model'), exist_ok=True)\n",
        "    model_weights_fname = os.path.join(cwd, 'model', f'{model_name}.h5')\n",
        "    model_json_fname = os.path.join(cwd, 'model', f'{model_name}.json')\n",
        "\n",
        "    # Load model and its weights\n",
        "    with open(model_json_fname, 'r') as f: \n",
        "        model_json = f.read()\n",
        "    model = model_from_json(model_json)\n",
        "    model.load_weights(model_weights_fname)\n",
        "\n",
        "    # Get MNIST data shaped appropriately for the model\n",
        "    (X_train, y_train), (X_test, y_test) = get_data(model)\n",
        "\n",
        "    # Compile model and evaluate its performance on training and test data\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    score = model.evaluate(X_train, y_train, verbose=0)\n",
        "    print()\n",
        "    print('Training loss:', score[0])\n",
        "    print('Training accuracy:', score[1])\n",
        "\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print()\n",
        "    print('Validation loss:', score[0])\n",
        "    print('Validation accuracy:', score[1])\n",
        "\n",
        "    # Determine validation examples that the model got wrong\n",
        "    y_pred = np.array([np.argmax(y) for y in model.predict(X_test)])\n",
        "    y_true = np.array([np.argmax(y) for y in y_test])\n",
        "    mistakes = (y_pred != y_true)\n",
        "    X_wrong = X_test[mistakes].reshape(-1, 28, 28) # To visualize properly\n",
        "    y_wrong = y_pred[mistakes]\n",
        "    y_right = y_true[mistakes]\n",
        "\n",
        "    # Visualize some of the validation examples the model got wrong\n",
        "    nrow, ncol = 3, 5\n",
        "    fig, axs = plt.subplots(nrows=nrow, ncols=ncol)\n",
        "    for i in range(nrow):\n",
        "        for j in range(ncol):\n",
        "            idx = i * ncol + j\n",
        "            ax = axs[i][j]\n",
        "            ax.imshow(X_wrong[idx], cmap='gray')\n",
        "            ax.set_title(f'Pred: {y_wrong[idx]}\\nTrue: {y_right[idx]}')\n",
        "            ax.axis('off')\n",
        "    fig.tight_layout(pad=2.5)\n",
        "    plt.suptitle(f'Validation Images {model_name} Got Wrong')\n",
        "    plt.savefig(os.path.join(cwd, 'img', f'{model_name}_mistakes.png'))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "main(FILE_PREFIX)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bonus_Part_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
